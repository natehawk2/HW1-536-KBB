---
title: "KW 1 KBB"
author: "Nathan Hawkins"
date: "9/14/2021"
output:
  pdf_document: default
  html_document: default
---
```{r include=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(bestglm)
library(corrplot)
library(scales)
library(tidyverse)

kbb <- read.csv("KBB.csv", stringsAsFactors = TRUE)

```
# Introduction
In this analysis we are trying to help consumers understand what factors influence the price of their cars, specifically understanding the relationship between car make and mileage of a car. For this analysis we are using data from Kelley Blue Books, a vehicle evaluation and automotive research company. The data contains 12 variables (Mileage, Make, Model, Trim, Type, Cylinder, Liter, Number of Doors, Cruise, Sound, Leather, and Price) including our response variable price and 804 observations. All of these variables, besides price and mileage, are factor variables. One issue that arises when looking at the data is that some of the factor variables are linear combination of other factors. An example of this is certain trims are only available for one type of model making it impossible to separate the effect of that specific trim and the model it is associated with. Another concern is some of the variables have upwards of 30 levels making it which might lead to inflated p-values when looking at the model.

#EDA

When looking at the effect mileage has on price it appears as though there might be different effects depending on the make of the car. We are using this interaction in our model. 

```{r Game-Effect, fig.cap="This graph shows the prmice of cars as a function of how many miles they have and what make the car is. The x axis is miles and the y axis is price is USD",fig.align='center', echo=FALSE, message=FALSE}


kbb %>% 
  ggplot(aes(x = Mileage, y = Price, color = Make)) +
  geom_point(alpha = 0.3) +
  theme_minimal() +
    labs(
    title = "Scatter Plot of Price as a Function of Mileage and Make",
    caption = "Data taken from the Kelley Blue Book",
    color = "Make"
  ) +
  # geom_smooth(data = (kbb %>% filter(Make == "Buick")), method = "lm", se = FALSE) +
  geom_smooth(data = (kbb %>% filter(Make == "Cadillac")), method = "lm", se = FALSE) +
  # geom_smooth(data = (kbb %>% filter(Make == "Chevrolet")), method = "lm", se = FALSE) +
  # geom_smooth(data = (kbb %>% filter(Make == "Pontiac")), method = "lm", se = FALSE) +
  # geom_smooth(data = (kbb %>% filter(Make == "SAAB")), method = "lm", se = FALSE) +
  geom_smooth(data = (kbb %>% filter(Make == "Saturn")), method = "lm", se = FALSE) +
  xlab("Mileage") + 
  ylab("") +
  scale_y_continuous(labels = label_number(suffix = "K", scale = 1e-3)) + 
  scale_x_continuous(labels = label_number(suffix = "K", scale = 1e-3)) +
  theme(
    panel.grid = element_blank(),
    axis.title.y = element_text(angle = 0),
    title = element_text(size = 15)
  )
```  



There also appears to be a difference between cars with leather and cars without and cars with upgraded sound system and those without

```{r echo = FALSE}


# ggplot(data = kbb, mapping = aes(x = Mileage, y = Price)) + 
#   geom_point() 
# 
# ggplot(data = kbb, mapping = aes(x = as.factor(Doors), y = Price)) + 
#   geom_boxplot() 
# 
# ggplot(data = kbb, mapping = aes(x = as.factor(Cylinder), y = Price)) + 
#   geom_boxplot() 
# 
# ggplot(data = kbb, mapping = aes(x = as.factor(Make), y = Price)) + 
#   geom_boxplot() 
# 
# ggplot(data = kbb, mapping = aes(y = as.factor(Trim), x = Price)) + 
#   geom_boxplot() 
# 
# ggplot(data = kbb, mapping = aes(y = as.factor(Model), x = Price)) + 
  # geom_boxplot() 

par(mfrow=c(1,2))

kbb %>% 
  mutate(Leather = case_when(
    Leather == 0 ~ "No Leather",
    TRUE ~ "Leather"
  )) %>% 
ggplot(mapping = aes(x = as.factor(Leather), y = Price, fill = as.factor(Leather))) + 
  geom_boxplot() +
  theme_minimal() +
  labs(
    x = "",
    y = "",
    title = "Boxplot Comparing Price of Cars with Leather to Cars without Non Leather",
    caption = "Data taken from the Kelley Blue Book",
    fill = ""
  ) +
  theme(
    legend.position = "None",
    panel.grid = element_blank(),
    axis.title.y = element_text(angle = 0)
  ) +
  scale_y_continuous(labels = label_number(suffix = "K", scale = 1e-3)) 

kbb %>% 
  mutate(Sound = case_when(
    Sound == 0 ~ "Regular",
    TRUE ~ "Upgraded"
  )) %>% 
ggplot(mapping = aes(x = as.factor(Sound), y = Price, fill = as.factor(Sound))) + 
  geom_boxplot() +
  theme_minimal() +
  labs(
    x = "",
    y = "",
    title = "Boxplot Comparing Price of Cars with Upgraded Sound to Cars with Regular",
    caption = "Data taken from the Kelley Blue Book",
    fill = ""
  ) +
  theme(
    legend.position = "None",
    panel.grid = element_blank(),
    axis.title.y = element_text(angle = 0)
  ) +
  scale_y_continuous(labels = label_number(suffix = "K", scale = 1e-3))

# ggplot(data = kbb, mapping = aes(x = as.factor(Cruise), y = Price)) + 
#   geom_boxplot()
```




```{r}
library(dplyr)


kbb$make_model_trim <- as.factor(paste0(kbb$Make, kbb$Model, kbb$Trim))

# library(bestglm)
# bestglm(kbb[,c(2:13,1)], IC = "AIC", method = "exhaustive")

model <- lm(Price ~  Mileage + Mileage:Make + Sound + Leather + make_model_trim , data = kbb)

AIC(model)
BIC(model)
rsq::rsq(model)
summary(model)
library(car)
vif(model)

sd(model$residuals)
sd(kbb$Price)

length(model$coefficients)

```

# Model Selection

When choosing which best model to use we by look at each every model with every permutation of variables, not including interactions. When we do this we discover that many of our best models in terms of AIC, BIC, and adjusted R squared included variables that were linear combinations of other variables. This means that many of the coefficients are now uninterpreted. After trying several different methods to solve this problem including, eliminating the variable, reducing the number of levels inside each variable, and filtering out data to make two separate models we decide that our best course of action was to combine the three variables of make, model, and trim into one variable. This eliminates the issue of linear combinations while still allowing us to interpret our coefficients. Also when it comes to making our final model it is important for us to include the interaction between make and mileage, as this is one of the specific questions asked by the consumer. After all of these considerations our final model is.


Model: $y_i$ = $\beta_0$ + $\beta_1$$x_{i1}$ +  $\beta_2$$x_{i2}$ + $\beta_3$$x_{i3}$ + $\beta_4$$x_{i4}$$x_{i2}$ + .... + $\beta_{82}$$x_{i82}$$x_{i3}$ + $\beta_{83}$$x_{i1}$$x_{i83}$ + $\beta_{84}$$x_{i1}$$x_{i84}$ + $\beta_{85}$$x_{i1}$$x_{i85}$ + $\beta_{86}$$x_{i1}$$x_{i86}$ + $\beta_{87}$$x_{i1}$$x_{i87}$ + $\epsilon_i$ or

$y_i$ = $\beta_0$ + $\beta_1$Mileage +  $\beta_2$I(group = Upgraded Sound) +  $\beta_3$I(group = Leather) + $\beta_4$I(group = Buick Lacrosse CX Sedan 4D) + .... + $\beta_82$I(group = SaturnL Series L300 Sedan 4D) + $\beta_83$Mileage * I(group = Cadillac) + $\beta_84$Mileage * I(group = Chevrolet) + $\beta_85$Mileage * I(group = Pontiac) + $\beta_86$Mileage * I(group = SAAB) + $\beta_87$Mileage * I(group = Saturn) + $\epsilon_i$

$\epsilon \sim \mathcal{N}(\mu,\,\sigma^{2})$

$\beta_0$: The cost of a Buick Century Sedan 4D with no upgraded sound system, no leather, and 0 miles.

$\beta_1$: This is the effect that increase by one has on the price of our reference make of Buick holding all else constant

$\beta_2$: This is the effect that adding an upgraded sound system has on the price of a car compared to one that does not

$\beta_3$: This is the effect that having a car with leather seats has on the price of a car compared to one that does not 

$\beta_{4-82}$: These are the effects of having a certain make, model, and trim has on the price of our reference make, model, and trim (Buick Century Sedan 4D)

$\beta_{83}$: This is the effect of the interaction of mileage with the make of a Cadillac

$\beta_{84}$: This is the effect of the interaction of mileage with the make of a Chevrolet

$\beta_{85}$: This is the effect of the interaction of mileage with the make of a Pontiac

$\beta_{86}$: This is the effect of the interaction of mileage with the make of a SAAB

$\beta_{87}$: This is the effect of the interaction of mileage with the make of a Saturn

$x_{i1}$: Number of miles driven in a car

$x_{i2}$: A binary indicator indicating if a car has an upgraded sound system

$x_{i3}$: A binary indicator indicating if a car has leather

$x_{i4-i82}$: Binary indicators indicating if a car is one of 78 make, model, and trim combinations.

$x_{i83}$: A binary indicator indicating if a car's make is Cadillac

$x_{i84}$: A binary indicator indicating if a car's make is Chevrolet

$x_{i85}$: A binary indicator indicating if a car's make is Pontiac

$x_{i86}$: A binary indicator indicating if a car's make is SAAB

$x_{i87}$: A binary indicator indicating if a car's make is Saturn

When using this model we make some assumptions being linearity, independence of observations, normal distributed residuals, and equal variance. These assumptions will be justified late on in the analysis.

```{r}
samp <- sample(1:nrow(kbb), round(nrow(kbb)*.9, 0), replace = FALSE)
train <- kbb[samp,]
test <- kbb[-samp,]

train.model <- lm(Price ~  Mileage + Mileage:Make + Sound + Leather + make_model_trim , data = train)

predictions <- predict.lm(train.model, test)

(test$Price - predictions)^2
```



```{r eval=FALSE}

summary(model)
library(multcomp)
library(car)
car::vif(model)
a <- matrix(rep(0, 66), nrow = 1)
a[45] <- 1
dim(a)
library(MASS)
dim(vcov(model))
glht_a <- glht(model, a)
summary(glht_a)
```


```{r include = FALSE, eval=FALSE}
kbb$Cylinder <- as.factor(kbb$Cylinder)
model <- Price~.
fit <- lm(model, kbb)
test <- olsrr::ols_step_all_possible(fit)
View(test)

```


# Assumptions

Linearity assumption.
```{r}
avPlots(model)
```

# Normality

They look very normal.
```{r}
hist(model$residuals)
qqPlot(model)
```
# Equal Variance

```{r}
plot(model$fitted.values, model$residuals)
plot(model)
```



```{r}
library(magrittr)
mydataset <- kbb
n <- round(nrow(mydataset),0)
n.cv <- 100 #Number of CV studies to run
n.test <-  round(nrow(mydataset)*0.1,0)
rpmse <- rep(x=NA, times=n.cv)
bias <- rep(x=NA, times=n.cv)
wid <- rep(x=NA, times=n.cv)
cvg <- rep(x=NA, times=n.cv)
for(cv in 1:n.cv){
  ## Select test observations
  test.obs <- sample(x=1:n, size=n.test)
  
  ## Split into test and training sets
  test.set <- mydataset[test.obs,]
  train.set <- mydataset[-test.obs,]
  
  ## Fit a lm() using the training data
  train.lm <- lm(Price ~  Mileage + Mileage:Make + Sound + Leather + make_model_trim , data = train.set)
  
  ## Generate predictions for the test set
  my.preds <- predict.lm(train.lm, newdata=test.set, interval="prediction")
  
  ## Calculate bias
  bias[cv] <- mean(my.preds[,'fit']-test.set[['Price']])
  
  ## Calculate RPMSE
  rpmse[cv] <- (test.set[['Price']]-my.preds[,'fit'])^2 %>% mean() %>% sqrt()
  
  ## Calculate Coverage
  cvg[cv] <- ((test.set[['Price']] > my.preds[,'lwr']) & (test.set[['Price']] < my.preds[,'upr'])) %>% mean()
  
  ## Calculate Width
  wid[cv] <- (my.preds[,'upr'] - my.preds[,'lwr']) %>% mean()
  
}
hist(rpmse)
hist(bias)
mean(bias)
hist(cvg)
hist(wid)
sd(kbb$Price)
```

